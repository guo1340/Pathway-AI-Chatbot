# choose your LLM provider: openai | ollama
LLM_PROVIDER=openai

# --- if LLM_PROVIDER=openai ---
OPENAI_API_KEY="sk-<OPEN_API_KEY>"
# optional; default gpt-4o-mini (cheap) or gpt-4o for better quality
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# --- if LLM_PROVIDER=ollama ---
# make sure `ollama serve` is running locally, e.g. `ollama run llama3.1`
OLLAMA_MODEL=llama3.1
# tip: for embeddings with ollama you can use "nomic-embed-text" pulled via ollama

# server + rag settings
PORT=8000
CORS_ORIGINS=http://localhost:5173,http://localhost:3000
DOCS_DIR=./docs
CHROMA_DIR=./chroma_store
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K=4
TEMPERATURE=0.2

CHROMA_DISABLE_TELEMETRY=1